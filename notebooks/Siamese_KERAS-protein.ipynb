{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://keras.io/examples/mnist_siamese/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "from time import time, strftime\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import ipyvolume as ipv\n",
    "import pathlib\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from cryoem.projections import RotationMatrix\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, AvgPool2D, Lambda\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow_graphics.util import shape\n",
    "from tensorflow_graphics.geometry.transformation import quaternion, euler\n",
    "from tensorflow_graphics.util import asserts\n",
    "from tensorflow_graphics.math import vector\n",
    "from tensorflow_graphics.util import safe_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6601222070488256641\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5394641197806668484\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3457499586955090808\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2740211166231494559\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11322146816\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 1129173418841313582\n",
      "physical_device_desc: \"device: 0, name: Tesla K40c, pci bus id: 0000:3b:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11322146816\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 9566690091567485082\n",
      "physical_device_desc: \"device: 1, name: Tesla K40c, pci bus id: 0000:86:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 projections of images with dimension (116, 116) pixels\n",
      "5000 sets of 3 ground truth angles of corresponding projection images\n"
     ]
    }
   ],
   "source": [
    "angle_ranges = \"2.0,0.4,2.0\"\n",
    "angle_shift = \"0.0,0.0,0.0\"\n",
    "num_projections = 5000\n",
    "dir_name = \"../data/\"\n",
    "projections_filename = f\"5j0n_ProjectionsAngles_ProjNber{num_projections}_AngCoverage{angle_ranges}_AngShift{angle_shift}\"\n",
    "\n",
    "\n",
    "# load structures\n",
    "data = h5py.File(os.path.join(dir_name, f\"{projections_filename}.h5\"), 'r')\n",
    "\n",
    "X, y = data['Projections'], data['Angles']\n",
    "print(f\"{X.shape[0]} projections of images with dimension {X.shape[1:]} pixels\")\n",
    "print(f\"{y.shape[0]} sets of {y.shape[1]} ground truth angles of corresponding projection images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1643e6e0e554309a1ea4c1804f34e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot coverage\n",
    "all_vectors = RotationMatrix(y)\n",
    "ipv.figure(width=500, height=500)\n",
    "ipv.pylab.xlim(-1, 1)\n",
    "ipv.pylab.ylim(-1, 1)\n",
    "ipv.pylab.zlim(-1, 1)\n",
    "ipv.scatter(all_vectors[:,0], all_vectors[:,2], all_vectors[:,1], marker=\"sphere\", color=\"blue\", size=1)\n",
    "ipv.pylab.save(f\"data/angle_variety/{angle_ranges}.html\")\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Angle Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler2quaternion(angles):\n",
    "    \"\"\"\n",
    "    Quaternion implements 3 rotations along x, y, z axis. \n",
    "    We compose them to get the final (single) rotation.\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.name_scope(None, \"quaternion_from_euler\", [angles]):\n",
    "        #print(angles)\n",
    "        a = [angles[i] for i in range(len(angles))]\n",
    "\n",
    "        a = tf.convert_to_tensor(value=a)\n",
    "\n",
    "        shape.check_static(tensor=a, tensor_name=\"angles\", has_dim_equals=(-1, 3))\n",
    "\n",
    "        half_angles = a / 2.0\n",
    "        cos_half_angles = tf.cos(half_angles)\n",
    "        sin_half_angles = tf.sin(half_angles)\n",
    "        c1, c2, c3 = tf.unstack(cos_half_angles, axis=-1)\n",
    "        s1, s2, s3 = tf.unstack(sin_half_angles, axis=-1)\n",
    "        # Tait-Bryan angles\n",
    "        #w = c1 * c2 * c3 + s1 * s2 * s3\n",
    "        #x = -c1 * s2 * s3 + s1 * c2 * c3\n",
    "        #y = c1 * s2 * c3 + s1 * c2 * s3\n",
    "        #z = -s1 * s2 * c3 + c1 * c2 * s3\n",
    "        \n",
    "        # Euler angles\n",
    "        w = c1*c2*c3 - s1*c2*s3\n",
    "        x = c1*s2*s3 - s1*s2*c3\n",
    "        y = c1*s2*c3 + s1*s2*s3\n",
    "        z = c1*c2*s3 + s1*c2*c3\n",
    "        return tf.stack((x, y, z, w), axis=-1)\n",
    "\n",
    "def d_q(q1, q2):\n",
    "     with (tf.compat.v1.name_scope(None, \"quaternion_relative_angle\",[q1, q2])):\n",
    "        q1 = tf.convert_to_tensor(value=q1)\n",
    "        q2 = tf.convert_to_tensor(value=q2)\n",
    "      \n",
    "        shape.check_static(\n",
    "            tensor=q1, tensor_name=\"quaternion1\", has_dim_equals=(-1, 4))\n",
    "        shape.check_static(\n",
    "            tensor=q2, tensor_name=\"quaternion2\", has_dim_equals=(-1, 4))\n",
    "\n",
    "        q1 = quaternion.normalize(q1)\n",
    "        q2 = quaternion.normalize(q2)\n",
    "        \n",
    "        dot_product = vector.dot(q1, q2, keepdims=False)\n",
    "        \n",
    "        # Ensure dot product is in range [-1. 1].\n",
    "        const = 1.8 #4.0 #.63\n",
    "        eps_dot_prod = const * asserts.select_eps_for_addition(dot_product.dtype)\n",
    "        dot_product = safe_ops.safe_shrink(\n",
    "            dot_product, -1, 1, open_bounds=False, eps=eps_dot_prod)\n",
    "\n",
    "        return 2.0 * tf.acos(tf.abs(dot_product)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (d_q(euler2quaternion([np.pi/2]*3), euler2quaternion([2*np.pi-np.pi/2]*3))-np.pi) < 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_p(p1, p2):\n",
    "    # (learned) distance between two images.\n",
    "    # for now, Euclid dist\n",
    "    p1 = tf.convert_to_tensor(value=p1, dtype=np.float64)\n",
    "    p2 = tf.convert_to_tensor(value=p2, dtype=np.float64)\n",
    "\n",
    "    if len(p1.shape) > 1:\n",
    "        dist = tf.norm(p1-p2, ord='euclidean', axis=1, keepdims=True)\n",
    "    else:\n",
    "        dist = tf.norm(p1-p2, ord='euclidean')\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.reduce_mean(d_p(X[0:3], X[0:3])) < 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN of the Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_output(k):\n",
    "    start_time = time()\n",
    "\n",
    "    _X = np.reshape(X, (X.shape[0], -1))\n",
    "    \n",
    "    name = projections_filename.split('/')[-1]\n",
    "    if not os.path.exists(f'data/knn{k}_{name}_distances.npy'):\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, metric=d_p, algorithm='ball_tree', n_jobs=-1).fit(_X)\n",
    "        distances_p, indices_p = nbrs.kneighbors(_X)\n",
    "        A_p = nbrs.kneighbors_graph(_X).toarray()\n",
    "\n",
    "        try:\n",
    "            np.save(f'data/knn{k}_{name}_indices', indices_p)         # Indices of the nearest points in the population matrix\n",
    "            np.save(f'data/knn{k}_{name}_distances', distances_p)     # Array representing the lengths to points\n",
    "            np.save(f'data/knn{k}_{name}_A', A_p)                     # Sparse graph showing the connections between neighboring points\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"--- {time() - start_time} seconds ---\")\n",
    "    \n",
    "    else:\n",
    "        indices_p     = np.load(f'data/knn{k}_{name}_indices.npy')     # shape: NUM_IMGS, NUM_NEIGHBOURS\n",
    "        distances_p   = np.load(f'data/knn{k}_{name}_distances.npy')   # shape: NUM_IMGS, NUM_NEIGHBOURS\n",
    "        A_p           = np.load(f'data/knn{k}_{name}_A.npy')           # shape: NUM_IMGS, NUM_IMGS\n",
    "\n",
    "    \n",
    "    return indices_p, distances_p, A_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 23594.572999954224 seconds ---\n",
    "indices_p, distances_p, A_p = get_knn_output(k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.95002548806251"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(distances_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pairs(num_projections, num_pairs, style=\"random\", k=None):\n",
    "    if not k and style != \"random\":\n",
    "        raise ValueError(\"Please specify k for kNN for sample_pairs method\")\n",
    "    \n",
    "    if style==\"random\":\n",
    "        idx1 = list(np.random.randint(0, num_projections, num_pairs))\n",
    "        idx2 = list(np.random.randint(0, num_projections, num_pairs))\n",
    "    \n",
    "    elif style==\"knn\":\n",
    "        idx1 = list(np.random.randint(0, num_projections, num_pairs))\n",
    "        indices_p, distances_p, A_p = get_knn_output(k=k)\n",
    "        idx2 = [indices_p[i][np.random.randint(1, k)] for i in idx1]\n",
    " \n",
    "    elif style==\"knn_and_random\":\n",
    "        # select random sample for the first element of pair\n",
    "        idx1 = list(np.random.randint(0, num_projections, num_pairs))\n",
    "        \n",
    "        # half from kNN\n",
    "        indices_p, distances_p, A_p = get_knn_output(k=k)\n",
    "        idx2_knn = [indices_p[i][np.random.randint(1, k)] for i in idx1[:num_pairs//2]]\n",
    "        idx2_random = list(np.random.randint(0, num_projections, num_pairs//2))\n",
    "        # half random\n",
    "        idx2 = idx2_knn + idx2_random\n",
    "        \n",
    "    return idx1, idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    return  K.mean(K.square(y_pred - y_true))\n",
    "\n",
    "def create_pairs(x, y, num_pairs):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Sample some pairs.\n",
    "    idx1, idx2 = sample_pairs(num_projections=NUM_PROJECTIONS, num_pairs=num_pairs, style=\"knn\", k=k)\n",
    "    \n",
    "    for z1, z2 in zip(idx1, idx2):\n",
    "        pairs += [[x[z1], x[z2]]]\n",
    "        labels += [d_q(euler2quaternion(y[z1]), euler2quaternion(y[z2]))]\n",
    "\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input_x = Input(shape=input_shape)\n",
    "\n",
    "    # add Convolution, MaxPool, Conv2D, remove Dropout and Dense\n",
    "    x = Conv2D(filters=32, kernel_size=[7, 7], activation='relu', padding='same', kernel_initializer='glorot_uniform')(input_x)\n",
    "    x = MaxPooling2D([2, 2], padding='same')(x)\n",
    "\n",
    "    x = Conv2D(64, [5, 5], activation='relu', padding='same', kernel_initializer='glorot_uniform')(x)\n",
    "    x = MaxPooling2D([2, 2], padding='same')(x)\n",
    "\n",
    "    x = Conv2D(128, [3, 3], activation='relu', padding='same', kernel_initializer='glorot_uniform')(x)\n",
    "    x = MaxPooling2D([2, 2], padding='same')(x)\n",
    "\n",
    "    x = Conv2D(256, [1, 1], activation='relu', padding='same', kernel_initializer='glorot_uniform')(x)\n",
    "    x = MaxPooling2D([2, 2], padding='same')(x)\n",
    "\n",
    "    x = AvgPool2D(pool_size=[8, 8], padding='same')(x)\n",
    "\n",
    "    x = tf.squeeze(x, axis=[1,2])\n",
    "    \n",
    "    return Model(input_x, x)\n",
    "\n",
    "\n",
    "# def accuracy(y_true, y_pred):\n",
    "#     '''Compute classification accuracy with a fixed threshold on distances.\n",
    "#     '''\n",
    "#     y_pred = K.flatten(y_pred)\n",
    "\n",
    "#     return K.mean(K.square(y_pred - y_true) < ACCURACY_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gloabal average pooling - width and hights OR \n",
    "flatten\n",
    "\n",
    "Pooling - shrinks in space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_accuracy(y_true, y_pred):\n",
    "#     '''Compute classification accuracy with a fixed threshold on distances.\n",
    "#     '''\n",
    "#     y_pred = K.flatten(y_pred)\n",
    "\n",
    "#     return K.mean(K.square(y_pred - y_true) < ACCURACY_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "NUM_PROJECTIONS = 5000\n",
    "NUM_PAIRS = 50000\n",
    "CHECKPOINT_PATH = f\"training/{strftime('%Y%m%d_%H%M%S')}\"\n",
    "pathlib.Path(CHECKPOINT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "TENSORBOARD_PATH = f\"logs/fit/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "pathlib.Path(TENSORBOARD_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "train_idx, test_idx, _, _ = train_test_split(range(NUM_PAIRS), \n",
    "                                             range(NUM_PAIRS), \n",
    "                                             test_size=0.33, \n",
    "                                             random_state=42)\n",
    "train_idx = sorted(train_idx)\n",
    "test_idx = sorted(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = create_pairs(X, y, NUM_PAIRS)\n",
    "\n",
    "tr_pairs, tr_y = X_data[train_idx], y_data[train_idx]\n",
    "tr_pairs = tr_pairs.reshape(list(tr_pairs.shape) + [-1])\n",
    "tr_y = tf.cast(tr_y, dtype=tf.float32)\n",
    "\n",
    "te_pairs, te_y = X_data[test_idx], y_data[test_idx]\n",
    "te_pairs = te_pairs.reshape(list(te_pairs.shape) + [-1])\n",
    "te_y = tf.cast(te_y, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = te_pairs[:, 0].shape[1:]#list(te_pairs[:, 0].shape[1:])\n",
    "print(f\"Input images shape {input_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pairs[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.15 loss, 0.0017 acc\n",
    "\n",
    "test set\n",
    "plot train and test losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# network definition\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "\n",
    "\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "\n",
    "# train\n",
    "rms = RMSprop()\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=rms, metrics=None)  #[accuracy])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "backup_callback = ModelCheckpoint(filepath=CHECKPOINT_PATH,\n",
    "                              save_weights_only=True,\n",
    "                              verbose=1)\n",
    "# Define the Keras TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(log_dir=TENSORBOARD_PATH)\n",
    "\n",
    "history = model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n",
    "                  callbacks=[backup_callback, tensorboard_callback])\n",
    "\n",
    "# %tensorboard --logdir logs\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute final accuracy on training and test sets\n",
    "y_tr_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "# tr_acc = compute_accuracy(tr_y, y_tr_pred)\n",
    "# print(f'* Accuracy on training set: {(100 * tr_acc):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "# te_acc = compute_accuracy(te_y, y_te_pred)\n",
    "# print(f'* Accuracy on test set: {(100 * te_acc):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_pairs = te_pairs.reshape(list(te_pairs.shape[:-2]) +[-1])\n",
    "tr_pairs = tr_pairs.reshape(list(tr_pairs.shape[:-2]) +[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imfig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(te_pairs[2, 0])\n",
    "ax2.imshow(te_pairs[2, 1])\n",
    "print(f\"--- Test Set ---\")\n",
    "print(f\"loss:      {loss(y_te_pred[0], te_y[0].numpy())}\")\n",
    "print(f\"predicted: {y_te_pred[0][0][0][0]}\")\n",
    "print(f\"true:      {te_y[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(tr_pairs[2, 0])\n",
    "ax2.imshow(tr_pairs[2, 1])\n",
    "print(f\"--- Train Set ---\")\n",
    "print(f\"loss:      {loss(y_tr_pred[0], tr_y[0].numpy())}\")\n",
    "print(f\"predicted: {y_tr_pred[0][0][0][0]}\")\n",
    "print(f\"true:      {tr_y[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
