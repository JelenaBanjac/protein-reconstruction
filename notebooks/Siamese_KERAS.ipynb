{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://keras.io/examples/mnist_siamese/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(NUM_CLASSES)]) - 1\n",
    "    for d in range(NUM_CLASSES):\n",
    "        for i in range(n):\n",
    "            # similar pair\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, NUM_CLASSES)\n",
    "            # dissimilar pair\n",
    "            dn = (d + inc) % NUM_CLASSES\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images shape (28, 28)\n",
      "Train on 108400 samples, validate on 17820 samples\n",
      "Epoch 1/20\n",
      "108400/108400 [==============================] - 8s 71us/sample - loss: 0.0956 - accuracy: 0.8873 - val_loss: 0.0434 - val_accuracy: 0.9567\n",
      "Epoch 2/20\n",
      "108400/108400 [==============================] - 6s 60us/sample - loss: 0.0399 - accuracy: 0.9626 - val_loss: 0.0296 - val_accuracy: 0.9688\n",
      "Epoch 3/20\n",
      "108400/108400 [==============================] - 7s 61us/sample - loss: 0.0277 - accuracy: 0.9730 - val_loss: 0.0279 - val_accuracy: 0.9693\n",
      "Epoch 4/20\n",
      "108400/108400 [==============================] - 7s 62us/sample - loss: 0.0222 - accuracy: 0.9780 - val_loss: 0.0273 - val_accuracy: 0.9681\n",
      "Epoch 5/20\n",
      "108400/108400 [==============================] - 7s 60us/sample - loss: 0.0184 - accuracy: 0.9813 - val_loss: 0.0238 - val_accuracy: 0.9725\n",
      "Epoch 6/20\n",
      "108400/108400 [==============================] - 7s 61us/sample - loss: 0.0161 - accuracy: 0.9839 - val_loss: 0.0243 - val_accuracy: 0.9717\n",
      "Epoch 7/20\n",
      "108400/108400 [==============================] - 6s 60us/sample - loss: 0.0145 - accuracy: 0.9849 - val_loss: 0.0229 - val_accuracy: 0.9722\n",
      "Epoch 8/20\n",
      "108400/108400 [==============================] - 7s 61us/sample - loss: 0.0131 - accuracy: 0.9863 - val_loss: 0.0232 - val_accuracy: 0.9735\n",
      "Epoch 9/20\n",
      "108400/108400 [==============================] - 7s 60us/sample - loss: 0.0120 - accuracy: 0.9878 - val_loss: 0.0221 - val_accuracy: 0.9743\n",
      "Epoch 10/20\n",
      "108400/108400 [==============================] - 7s 61us/sample - loss: 0.0110 - accuracy: 0.9888 - val_loss: 0.0221 - val_accuracy: 0.9749\n",
      "Epoch 11/20\n",
      "108400/108400 [==============================] - 7s 60us/sample - loss: 0.0107 - accuracy: 0.9890 - val_loss: 0.0228 - val_accuracy: 0.9732\n",
      "Epoch 12/20\n",
      "108400/108400 [==============================] - 7s 60us/sample - loss: 0.0103 - accuracy: 0.9891 - val_loss: 0.0230 - val_accuracy: 0.9733\n",
      "Epoch 13/20\n",
      "108400/108400 [==============================] - 6s 60us/sample - loss: 0.0097 - accuracy: 0.9899 - val_loss: 0.0220 - val_accuracy: 0.9749\n",
      "Epoch 14/20\n",
      "108400/108400 [==============================] - 7s 60us/sample - loss: 0.0092 - accuracy: 0.9905 - val_loss: 0.0224 - val_accuracy: 0.9750\n",
      "Epoch 15/20\n",
      "108400/108400 [==============================] - 7s 61us/sample - loss: 0.0089 - accuracy: 0.9908 - val_loss: 0.0221 - val_accuracy: 0.9753\n",
      "Epoch 16/20\n",
      "108400/108400 [==============================] - 6s 60us/sample - loss: 0.0090 - accuracy: 0.9908 - val_loss: 0.0215 - val_accuracy: 0.9748\n",
      "Epoch 17/20\n",
      "108400/108400 [==============================] - 6s 59us/sample - loss: 0.0087 - accuracy: 0.9910 - val_loss: 0.0235 - val_accuracy: 0.9736\n",
      "Epoch 18/20\n",
      "108400/108400 [==============================] - 6s 60us/sample - loss: 0.0085 - accuracy: 0.9912 - val_loss: 0.0238 - val_accuracy: 0.9733\n",
      "Epoch 19/20\n",
      "108400/108400 [==============================] - 7s 60us/sample - loss: 0.0082 - accuracy: 0.9916 - val_loss: 0.0218 - val_accuracy: 0.9757\n",
      "Epoch 20/20\n",
      "108400/108400 [==============================] - 7s 60us/sample - loss: 0.0078 - accuracy: 0.9920 - val_loss: 0.0235 - val_accuracy: 0.9738\n",
      "* Accuracy on training set: 99.66\n",
      "* Accuracy on test set: 97.36\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "input_shape = x_train.shape[1:]\n",
    "print(f\"Input images shape {input_shape}\")\n",
    "\n",
    "\n",
    "# create training+test positive and negative pairs\n",
    "digit_indices = [np.where(y_train == i)[0] for i in range(NUM_CLASSES)]\n",
    "tr_pairs, tr_y = create_pairs(x_train, digit_indices)\n",
    "tr_y = tf.cast(tr_y, dtype=tf.float32)\n",
    "\n",
    "\n",
    "digit_indices = [np.where(y_test == i)[0] for i in range(NUM_CLASSES)]\n",
    "te_pairs, te_y = create_pairs(x_test, digit_indices)\n",
    "te_y = tf.cast(te_y, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# network definition\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "\n",
    "\n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "\n",
    "# train\n",
    "rms = RMSprop()\n",
    "\n",
    "\n",
    "model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n",
    "\n",
    "\n",
    "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n",
    "\n",
    "# compute final accuracy on training and test sets\n",
    "y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
    "tr_acc = compute_accuracy(tr_y, y_pred)\n",
    "y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(te_y, y_pred)\n",
    "\n",
    "print(f'* Accuracy on training set: {(100 * tr_acc):.2f}')\n",
    "print(f'* Accuracy on test set: {(100 * te_acc):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10839"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_images_per_class = int(tr_pairs[:, 0].shape[0]/10 - 1)\n",
    "num_images_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7fd43b6198>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(tr_pairs[num_images_per_class-3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7fd4340940>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tr_pairs[num_images_per_class-3, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
